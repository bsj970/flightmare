Scalar TrackerQuadrotorEnv::rewardFunction()
{
  // Outter coefficient
  Scalar c1 = 0.0;
  Scalar c2 = 1.0;
  Scalar c3 = -1e-4;

  // Range weight
  std::vector<Scalar> numerator;
  Scalar denominator = 0.0;
  for (int i = 0; i < num_targets_; ++i) {
    Vector<3> position = target_kalman_filters_[i]->getEstimatedPosition();
    Scalar distance = computeEuclideanDistance(quad_state_.p, position);
    Scalar elem = exp(-distance * 1.5);
    numerator.push_back(elem);
    denominator += elem;
  }

  if (std::isnan(denominator)) {
    std::cout << "nan occurs from individual denominator" << std::endl;
    std::cout << "denominator : " << denominator << std::endl;
    exit(0);
  }

  // Compute negative softmax
  std::vector<Scalar> range_weight;
  for (int i = 0; i < num_targets_; ++i) {
    Scalar weight = numerator[i] / denominator;

    if (std::isnan(weight)) {
      std::cout << "nan occurs from individual weight" << std::endl;
      std::cout << "weight : " << weight << std::endl;
      exit(0);
    }

    range_weight.push_back(weight);
  }

  // Heading reward
  Scalar heading_reward = 0.0;
  Vector<3> h = quad_state_.q().toRotationMatrix() * Vector<3>(1, 0, 0); // Ego tracker heading vector
  h = h / (h.norm() + 1e-8);
  for (int i = 0; i < num_targets_; ++i) {
    Vector<3> target_position = target_kalman_filters_[i]->getEstimatedPosition();
    Vector<3> d = target_position - quad_state_.p; // Relative distance to target
    d = d / (d.norm() + 1e-8);
    // Scalar theta = acos(h.dot(d));

    Scalar dot_value = h.dot(d);
    dot_value = std::max(static_cast<Scalar>(-1.0), std::min(static_cast<Scalar>(1.0), dot_value));
    Scalar theta = acos(dot_value);


    if (std::isnan(theta)) {
      std::cout << "nan occurs from individual theta" << std::endl;
      std::cout << "theta : " << theta << std::endl;
      std::cout << "dot_value : " << dot_value << std::endl;
      std::cout << "h : " << h << std::endl;
      std::cout << "d : " << d << std::endl;
      exit(0);
    }

    Scalar target_heading_reward = exp(-10.0 * pow(theta, 3));
    heading_reward += range_weight[i] * target_heading_reward;
  }


  // // 2. Target Covariance reward
  // Scalar avg_position_cov_norm = 0.0;
  // for (int i = 0; i < num_targets_; ++i) {
  //   Matrix<3, 3> position_cov = target_kalman_filters_[i]->getPositionErrorCovariance();
  //   avg_position_cov_norm += position_cov.norm();
  // }
  // avg_position_cov_norm /= num_targets_;
  // Scalar cov_reward = exp(-0.1 * pow(avg_position_cov_norm, 5));


  // 2. New Covariance reward
  Scalar cov_reward = 0.0;
  for (int i = 0; i < num_targets_; ++i) {
    Matrix<3, 3> cov = target_kalman_filters_[i]->getPositionErrorCovariance();
    Scalar target_cov_norm = cov.norm();
    Scalar target_cov_reward = exp(-0.01 * pow(target_cov_norm, 2));

    // std::cout << i << " target cov norm   : " << cov.norm() << std::endl;
    // std::cout << i << " target cov reward : " << target_cov_reward << std::endl;

    cov_reward += range_weight[i] * target_cov_reward;
  }

  // std::cout << "cov reward : " << cov_reward << std::endl;



  // 3. Smooth action reward (penalty)
  Scalar cmd_reward = pow((quad_act_ - prev_act_).norm(), 2);
  prev_act_ = quad_act_;

  Scalar total_reward = c1 * heading_reward + c2 * cov_reward + c3 * cmd_reward;

  // std::cout << "-------------------------------------" << std::endl;
  // std::cout << "heading reward : " << c1 * heading_reward << std::endl;
  // std::cout << "cov reward     : " << c2 * cov_reward << std::endl;
  // std::cout << "cmd reward     : " << c3 * cmd_reward << std::endl;
  // std::cout << "total reward   : " << total_reward << std::endl;

  return total_reward;
}